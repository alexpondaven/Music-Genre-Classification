{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training models on Covariance matrix and mean matrix of the MFCC of audio samples**\n",
    "\n",
    "The Covariance matrix and mean matrix was found in the Music Genre Classification Notebook, and will be used to train several models here.\n",
    "\n",
    "Models:\n",
    "- Logistic Regression\n",
    "- Convolutional Neural Network\n",
    "    - Did not work well using the covariance matrix, so the Mel Spectrogram was used in the \"Models using Mel Spectrogram\" notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split, validation_curve, learning_curve\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import math, random, pickle, os, operator\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "## Use GPU support\n",
    "# needed to prevent error from using too much gpu memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data - could try converting to pandas Dataframe instead\n",
    "dataset = []\n",
    "train = []\n",
    "test = []\n",
    "with open(\"feat.dat\", 'rb') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            dataset.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            f.close()\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to Pandas DataFrame\n",
    "flattened = []\n",
    "for x in dataset:   \n",
    "    flattened.append(list(x[0]) + list(x[1].flatten()) + [x[2]])\n",
    "\n",
    "cols = [['m' +str(i) for i in range(13)]+['c'+str(i) for i in range(169)]+[\"Genre\"]]\n",
    "df = pd.DataFrame(flattened, columns=cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>m0</th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>m3</th>\n",
       "      <th>m4</th>\n",
       "      <th>m5</th>\n",
       "      <th>m6</th>\n",
       "      <th>m7</th>\n",
       "      <th>m8</th>\n",
       "      <th>m9</th>\n",
       "      <th>...</th>\n",
       "      <th>c160</th>\n",
       "      <th>c161</th>\n",
       "      <th>c162</th>\n",
       "      <th>c163</th>\n",
       "      <th>c164</th>\n",
       "      <th>c165</th>\n",
       "      <th>c166</th>\n",
       "      <th>c167</th>\n",
       "      <th>c168</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.502611</td>\n",
       "      <td>-1.961417</td>\n",
       "      <td>-15.774347</td>\n",
       "      <td>3.831419</td>\n",
       "      <td>-10.473326</td>\n",
       "      <td>1.311828</td>\n",
       "      <td>-19.393732</td>\n",
       "      <td>5.286790</td>\n",
       "      <td>-16.631725</td>\n",
       "      <td>5.353444</td>\n",
       "      <td>...</td>\n",
       "      <td>3.786696</td>\n",
       "      <td>-0.021175</td>\n",
       "      <td>-14.523117</td>\n",
       "      <td>-12.995331</td>\n",
       "      <td>-3.843489</td>\n",
       "      <td>-14.831619</td>\n",
       "      <td>2.251195</td>\n",
       "      <td>29.282607</td>\n",
       "      <td>64.650762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66.931650</td>\n",
       "      <td>0.718853</td>\n",
       "      <td>-3.267830</td>\n",
       "      <td>4.181200</td>\n",
       "      <td>-8.050941</td>\n",
       "      <td>6.448259</td>\n",
       "      <td>-17.768517</td>\n",
       "      <td>14.091810</td>\n",
       "      <td>-18.332536</td>\n",
       "      <td>3.685560</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.026806</td>\n",
       "      <td>-13.013754</td>\n",
       "      <td>18.476615</td>\n",
       "      <td>11.603178</td>\n",
       "      <td>-3.788941</td>\n",
       "      <td>-17.738734</td>\n",
       "      <td>-8.665845</td>\n",
       "      <td>18.410328</td>\n",
       "      <td>90.762876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.774103</td>\n",
       "      <td>3.039700</td>\n",
       "      <td>-19.024435</td>\n",
       "      <td>-0.983591</td>\n",
       "      <td>-14.699337</td>\n",
       "      <td>7.586562</td>\n",
       "      <td>-12.823053</td>\n",
       "      <td>1.892920</td>\n",
       "      <td>-14.865326</td>\n",
       "      <td>3.595252</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.087720</td>\n",
       "      <td>-6.627783</td>\n",
       "      <td>-2.016122</td>\n",
       "      <td>-12.890326</td>\n",
       "      <td>-3.702126</td>\n",
       "      <td>-3.862404</td>\n",
       "      <td>0.221521</td>\n",
       "      <td>-9.624113</td>\n",
       "      <td>83.777252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.696107</td>\n",
       "      <td>9.211514</td>\n",
       "      <td>-4.532876</td>\n",
       "      <td>5.027148</td>\n",
       "      <td>-8.254011</td>\n",
       "      <td>8.101201</td>\n",
       "      <td>-9.676610</td>\n",
       "      <td>9.824043</td>\n",
       "      <td>-6.735213</td>\n",
       "      <td>10.332956</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.272778</td>\n",
       "      <td>-13.864303</td>\n",
       "      <td>13.764830</td>\n",
       "      <td>4.091590</td>\n",
       "      <td>-5.564044</td>\n",
       "      <td>-11.718776</td>\n",
       "      <td>-10.097750</td>\n",
       "      <td>4.902070</td>\n",
       "      <td>69.758813</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.972846</td>\n",
       "      <td>0.506410</td>\n",
       "      <td>-23.834838</td>\n",
       "      <td>-2.181944</td>\n",
       "      <td>-29.875498</td>\n",
       "      <td>0.719521</td>\n",
       "      <td>-19.039014</td>\n",
       "      <td>-1.353935</td>\n",
       "      <td>-13.370416</td>\n",
       "      <td>5.069944</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.531564</td>\n",
       "      <td>9.558282</td>\n",
       "      <td>14.068872</td>\n",
       "      <td>-4.271712</td>\n",
       "      <td>-29.419962</td>\n",
       "      <td>13.653176</td>\n",
       "      <td>6.385477</td>\n",
       "      <td>7.264809</td>\n",
       "      <td>102.319498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          m0        m1         m2        m3         m4        m5         m6  \\\n",
       "0  76.502611 -1.961417 -15.774347  3.831419 -10.473326  1.311828 -19.393732   \n",
       "1  66.931650  0.718853  -3.267830  4.181200  -8.050941  6.448259 -17.768517   \n",
       "2  78.774103  3.039700 -19.024435 -0.983591 -14.699337  7.586562 -12.823053   \n",
       "3  66.696107  9.211514  -4.532876  5.027148  -8.254011  8.101201  -9.676610   \n",
       "4  71.972846  0.506410 -23.834838 -2.181944 -29.875498  0.719521 -19.039014   \n",
       "\n",
       "          m7         m8         m9  ...       c160       c161       c162  \\\n",
       "0   5.286790 -16.631725   5.353444  ...   3.786696  -0.021175 -14.523117   \n",
       "1  14.091810 -18.332536   3.685560  ... -16.026806 -13.013754  18.476615   \n",
       "2   1.892920 -14.865326   3.595252  ... -18.087720  -6.627783  -2.016122   \n",
       "3   9.824043  -6.735213  10.332956  ... -21.272778 -13.864303  13.764830   \n",
       "4  -1.353935 -13.370416   5.069944  ... -15.531564   9.558282  14.068872   \n",
       "\n",
       "        c163       c164       c165       c166       c167        c168 Genre  \n",
       "0 -12.995331  -3.843489 -14.831619   2.251195  29.282607   64.650762     1  \n",
       "1  11.603178  -3.788941 -17.738734  -8.665845  18.410328   90.762876     1  \n",
       "2 -12.890326  -3.702126  -3.862404   0.221521  -9.624113   83.777252     1  \n",
       "3   4.091590  -5.564044 -11.718776 -10.097750   4.902070   69.758813     1  \n",
       "4  -4.271712 -29.419962  13.653176   6.385477   7.264809  102.319498     1  \n",
       "\n",
       "[5 rows x 183 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model now has 183 features and 1000 training examples\n",
    "- 13 - mean for each cepstral coefficient\n",
    "- 169 - elements of 13x13 covariance matrix\n",
    "- 1 - genre label (1-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE ANALYSIS**\n",
    "\n",
    "\n",
    "- Split data into training and test data (and randomize)\n",
    "- normalise values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df[[\"Genre\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation\n",
    "# standard scaler performs better than min max processor - between 0 and 1\n",
    "# - probably since it preserves the negative and positive values\n",
    "# - it may also converge quicker\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "# apply normalizer\n",
    "X_norm = pd.DataFrame(X_scaled)\n",
    "\n",
    "X_norm.columns = X.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Genre\n",
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "..    ...\n",
       "995    10\n",
       "996    10\n",
       "997    10\n",
       "998    10\n",
       "999    10\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data with separate labels:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm,y, test_size=0.1, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>m0</th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>m3</th>\n",
       "      <th>m4</th>\n",
       "      <th>m5</th>\n",
       "      <th>m6</th>\n",
       "      <th>m7</th>\n",
       "      <th>m8</th>\n",
       "      <th>m9</th>\n",
       "      <th>...</th>\n",
       "      <th>c159</th>\n",
       "      <th>c160</th>\n",
       "      <th>c161</th>\n",
       "      <th>c162</th>\n",
       "      <th>c163</th>\n",
       "      <th>c164</th>\n",
       "      <th>c165</th>\n",
       "      <th>c166</th>\n",
       "      <th>c167</th>\n",
       "      <th>c168</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.324414</td>\n",
       "      <td>-0.129911</td>\n",
       "      <td>0.451255</td>\n",
       "      <td>-2.100491</td>\n",
       "      <td>0.069541</td>\n",
       "      <td>-2.735649</td>\n",
       "      <td>2.174495</td>\n",
       "      <td>-2.143841</td>\n",
       "      <td>-0.503978</td>\n",
       "      <td>-0.326778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109965</td>\n",
       "      <td>-1.395237</td>\n",
       "      <td>0.545093</td>\n",
       "      <td>0.128219</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>4.121626</td>\n",
       "      <td>-0.679685</td>\n",
       "      <td>-0.962150</td>\n",
       "      <td>1.360786</td>\n",
       "      <td>1.708399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.249725</td>\n",
       "      <td>-0.542112</td>\n",
       "      <td>-0.738699</td>\n",
       "      <td>1.657118</td>\n",
       "      <td>-0.028496</td>\n",
       "      <td>0.436719</td>\n",
       "      <td>-0.779975</td>\n",
       "      <td>0.916687</td>\n",
       "      <td>0.301654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609829</td>\n",
       "      <td>-0.114878</td>\n",
       "      <td>-1.828992</td>\n",
       "      <td>-0.314283</td>\n",
       "      <td>0.251265</td>\n",
       "      <td>-0.521169</td>\n",
       "      <td>0.244801</td>\n",
       "      <td>-0.321018</td>\n",
       "      <td>-0.269711</td>\n",
       "      <td>-0.209186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>0.504429</td>\n",
       "      <td>-1.120477</td>\n",
       "      <td>2.051930</td>\n",
       "      <td>-0.705474</td>\n",
       "      <td>1.430730</td>\n",
       "      <td>-0.354240</td>\n",
       "      <td>1.246680</td>\n",
       "      <td>0.115724</td>\n",
       "      <td>1.408088</td>\n",
       "      <td>-1.592473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728586</td>\n",
       "      <td>1.839018</td>\n",
       "      <td>0.911773</td>\n",
       "      <td>1.057311</td>\n",
       "      <td>0.729928</td>\n",
       "      <td>-0.557491</td>\n",
       "      <td>0.418238</td>\n",
       "      <td>0.848465</td>\n",
       "      <td>0.573685</td>\n",
       "      <td>0.287174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.182115</td>\n",
       "      <td>-0.265140</td>\n",
       "      <td>-0.390672</td>\n",
       "      <td>-0.373524</td>\n",
       "      <td>-0.785140</td>\n",
       "      <td>0.614134</td>\n",
       "      <td>0.414218</td>\n",
       "      <td>0.876076</td>\n",
       "      <td>-0.907639</td>\n",
       "      <td>1.575361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.740360</td>\n",
       "      <td>-0.978161</td>\n",
       "      <td>0.540201</td>\n",
       "      <td>3.036502</td>\n",
       "      <td>0.419780</td>\n",
       "      <td>-0.993932</td>\n",
       "      <td>0.637347</td>\n",
       "      <td>0.396673</td>\n",
       "      <td>-0.319894</td>\n",
       "      <td>0.101487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>-1.418247</td>\n",
       "      <td>0.958260</td>\n",
       "      <td>-1.759171</td>\n",
       "      <td>-0.495696</td>\n",
       "      <td>-1.217841</td>\n",
       "      <td>-0.531072</td>\n",
       "      <td>-0.991686</td>\n",
       "      <td>0.301502</td>\n",
       "      <td>-0.680882</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244109</td>\n",
       "      <td>-0.407230</td>\n",
       "      <td>0.097293</td>\n",
       "      <td>0.569101</td>\n",
       "      <td>0.552478</td>\n",
       "      <td>0.474079</td>\n",
       "      <td>-1.559474</td>\n",
       "      <td>-1.192572</td>\n",
       "      <td>1.418135</td>\n",
       "      <td>1.720505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0.270112</td>\n",
       "      <td>-0.356234</td>\n",
       "      <td>1.924927</td>\n",
       "      <td>-1.657343</td>\n",
       "      <td>1.264483</td>\n",
       "      <td>-1.538151</td>\n",
       "      <td>1.714976</td>\n",
       "      <td>-1.761312</td>\n",
       "      <td>0.982302</td>\n",
       "      <td>0.617699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.736642</td>\n",
       "      <td>-0.563758</td>\n",
       "      <td>0.396159</td>\n",
       "      <td>0.905677</td>\n",
       "      <td>0.908408</td>\n",
       "      <td>0.314739</td>\n",
       "      <td>0.955612</td>\n",
       "      <td>0.301354</td>\n",
       "      <td>-0.734775</td>\n",
       "      <td>-0.578525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>0.836979</td>\n",
       "      <td>-0.774057</td>\n",
       "      <td>-0.041764</td>\n",
       "      <td>-1.201773</td>\n",
       "      <td>0.085740</td>\n",
       "      <td>-1.756904</td>\n",
       "      <td>0.203984</td>\n",
       "      <td>-0.810223</td>\n",
       "      <td>0.836356</td>\n",
       "      <td>-0.596034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.813021</td>\n",
       "      <td>-0.596388</td>\n",
       "      <td>-0.405800</td>\n",
       "      <td>0.248337</td>\n",
       "      <td>0.177381</td>\n",
       "      <td>-0.720140</td>\n",
       "      <td>0.373762</td>\n",
       "      <td>1.051508</td>\n",
       "      <td>-0.911721</td>\n",
       "      <td>-1.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.102735</td>\n",
       "      <td>-0.759999</td>\n",
       "      <td>-0.267127</td>\n",
       "      <td>0.099368</td>\n",
       "      <td>-0.369777</td>\n",
       "      <td>1.200677</td>\n",
       "      <td>-1.124899</td>\n",
       "      <td>0.905220</td>\n",
       "      <td>-0.687332</td>\n",
       "      <td>0.716586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213379</td>\n",
       "      <td>0.506083</td>\n",
       "      <td>0.126175</td>\n",
       "      <td>0.223781</td>\n",
       "      <td>0.019575</td>\n",
       "      <td>-0.531011</td>\n",
       "      <td>-0.113476</td>\n",
       "      <td>0.215231</td>\n",
       "      <td>-0.178004</td>\n",
       "      <td>-0.898507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.359062</td>\n",
       "      <td>-0.662445</td>\n",
       "      <td>1.059405</td>\n",
       "      <td>0.595044</td>\n",
       "      <td>-3.278725</td>\n",
       "      <td>-0.294056</td>\n",
       "      <td>-1.555517</td>\n",
       "      <td>-0.298002</td>\n",
       "      <td>0.544868</td>\n",
       "      <td>-1.636101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025403</td>\n",
       "      <td>1.066014</td>\n",
       "      <td>0.991613</td>\n",
       "      <td>0.084713</td>\n",
       "      <td>0.092987</td>\n",
       "      <td>0.363359</td>\n",
       "      <td>0.991457</td>\n",
       "      <td>0.596062</td>\n",
       "      <td>0.133435</td>\n",
       "      <td>-0.883249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>-0.298111</td>\n",
       "      <td>1.196861</td>\n",
       "      <td>-2.022751</td>\n",
       "      <td>2.444343</td>\n",
       "      <td>0.379646</td>\n",
       "      <td>-1.076444</td>\n",
       "      <td>0.285958</td>\n",
       "      <td>-2.780838</td>\n",
       "      <td>0.274988</td>\n",
       "      <td>-2.138301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176317</td>\n",
       "      <td>0.127409</td>\n",
       "      <td>1.137283</td>\n",
       "      <td>-0.363799</td>\n",
       "      <td>-0.486489</td>\n",
       "      <td>-0.279214</td>\n",
       "      <td>-0.322748</td>\n",
       "      <td>0.418335</td>\n",
       "      <td>-0.290681</td>\n",
       "      <td>-1.100755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           m0        m1        m2        m3        m4        m5        m6  \\\n",
       "753  0.324414 -0.129911  0.451255 -2.100491  0.069541 -2.735649  2.174495   \n",
       "894  0.413000  0.249725 -0.542112 -0.738699  1.657118 -0.028496  0.436719   \n",
       "852  0.504429 -1.120477  2.051930 -0.705474  1.430730 -0.354240  1.246680   \n",
       "346  0.182115 -0.265140 -0.390672 -0.373524 -0.785140  0.614134  0.414218   \n",
       "129 -1.418247  0.958260 -1.759171 -0.495696 -1.217841 -0.531072 -0.991686   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "799  0.270112 -0.356234  1.924927 -1.657343  1.264483 -1.538151  1.714976   \n",
       "575  0.836979 -0.774057 -0.041764 -1.201773  0.085740 -1.756904  0.203984   \n",
       "390  0.102735 -0.759999 -0.267127  0.099368 -0.369777  1.200677 -1.124899   \n",
       "72   0.359062 -0.662445  1.059405  0.595044 -3.278725 -0.294056 -1.555517   \n",
       "945 -0.298111  1.196861 -2.022751  2.444343  0.379646 -1.076444  0.285958   \n",
       "\n",
       "           m7        m8        m9  ...      c159      c160      c161  \\\n",
       "753 -2.143841 -0.503978 -0.326778  ...  0.109965 -1.395237  0.545093   \n",
       "894 -0.779975  0.916687  0.301654  ...  0.609829 -0.114878 -1.828992   \n",
       "852  0.115724  1.408088 -1.592473  ...  0.728586  1.839018  0.911773   \n",
       "346  0.876076 -0.907639  1.575361  ... -0.740360 -0.978161  0.540201   \n",
       "129  0.301502 -0.680882  0.595745  ...  0.244109 -0.407230  0.097293   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "799 -1.761312  0.982302  0.617699  ... -0.736642 -0.563758  0.396159   \n",
       "575 -0.810223  0.836356 -0.596034  ... -0.813021 -0.596388 -0.405800   \n",
       "390  0.905220 -0.687332  0.716586  ...  0.213379  0.506083  0.126175   \n",
       "72  -0.298002  0.544868 -1.636101  ...  0.025403  1.066014  0.991613   \n",
       "945 -2.780838  0.274988 -2.138301  ... -0.176317  0.127409  1.137283   \n",
       "\n",
       "         c162      c163      c164      c165      c166      c167      c168  \n",
       "753  0.128219  0.000742  4.121626 -0.679685 -0.962150  1.360786  1.708399  \n",
       "894 -0.314283  0.251265 -0.521169  0.244801 -0.321018 -0.269711 -0.209186  \n",
       "852  1.057311  0.729928 -0.557491  0.418238  0.848465  0.573685  0.287174  \n",
       "346  3.036502  0.419780 -0.993932  0.637347  0.396673 -0.319894  0.101487  \n",
       "129  0.569101  0.552478  0.474079 -1.559474 -1.192572  1.418135  1.720505  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "799  0.905677  0.908408  0.314739  0.955612  0.301354 -0.734775 -0.578525  \n",
       "575  0.248337  0.177381 -0.720140  0.373762  1.051508 -0.911721 -1.246000  \n",
       "390  0.223781  0.019575 -0.531011 -0.113476  0.215231 -0.178004 -0.898507  \n",
       "72   0.084713  0.092987  0.363359  0.991457  0.596062  0.133435 -0.883249  \n",
       "945 -0.363799 -0.486489 -0.279214 -0.322748  0.418335 -0.290681 -1.100755  \n",
       "\n",
       "[900 rows x 182 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Models</strong>\n",
    "\n",
    "**Logistic Regression**\n",
    "- Accuracy of 78% using:\n",
    "\n",
    "LogisticRegression(random_state=2, solver='lbfgs', multi_class='ovr', max_iter=8000, C=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, max_iter=1000, multi_class='ovr', n_jobs=-1,\n",
       "                   random_state=2)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit model\n",
    "# lbfgs solver used for multiclass problem - quicker than saga\n",
    "# \n",
    "log_regr = LogisticRegression(n_jobs=-1,random_state=2, solver='lbfgs', multi_class='ovr', max_iter=1000, C=0.01)\n",
    "log_regr.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important features:  []\n"
     ]
    }
   ],
   "source": [
    "# check importance of features on average\n",
    "weights = log_regr.coef_.sum(axis=0)\n",
    "imp = abs(weights) > 1\n",
    "impl = []\n",
    "for i in range(len(imp)):\n",
    "    if imp[i]:\n",
    "        impl.append(cols[0][i])\n",
    "\n",
    "print(\"Important features: \",impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using validation curve\n",
    "def plot_learning_curve(model):\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(model, X_train, y_train.values.ravel(),\n",
    "                                                            train_sizes=np.linspace(150,700,50, dtype=int),cv=5,\n",
    "                                                            scoring = 'neg_mean_squared_error')\n",
    "    train_scores_mean = -train_scores.mean(axis=1)\n",
    "    valid_scores_mean =-valid_scores.mean(axis=1)\n",
    "    # plot learning curves\n",
    "    plt.plot(train_sizes, train_scores_mean, label = \"Training error\")\n",
    "    plt.plot(train_sizes, valid_scores_mean, label = \"Validation error\")\n",
    "    plt.xlabel(\"Training size\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Learning Curve Logistic Regression Model\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(log_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy:  0.78\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "def accuracy(model):\n",
    "    pred = model.predict(X_test)\n",
    "    return round(sum(y_test.values.ravel() == pred) / len(y_test), 6)\n",
    "\n",
    "# Accuracy of logistic Regression model\n",
    "print(\"Logistic Regression Model Accuracy: \", accuracy(log_regr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively:\n",
    "log_regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting**\n",
    "\n",
    "From the learning curve above, it is apparent that more training data could help the validation curve converge towards the training curve and decrease error. This is done in the \"Adding more data\" notebook (or below).\n",
    "\n",
    "- increasing the strength of regularisation (decreasing C from 1 to 0.01) helped overfitting (accuracy went from 71% to 76%)\n",
    "\n",
    "\n",
    "Could try to decrease the number of features through PCA\n",
    "- could use PCA to find the K most significant components of the coefficient matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will only carry out PCA on MFCC coefficient matrix features\n",
    "X_m = X.iloc[:,:13]\n",
    "X_c = X.iloc[:,13:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to almost flatten out the 169 covariance matrix (13x13) into 13 principal components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_accuracy = [];\n",
    "\n",
    "def eval_PCA(model, K):\n",
    "\n",
    "    pca = PCA(n_components=K)\n",
    "    principal_components = pca.fit_transform(X_c)\n",
    "    pcaDf = pd.DataFrame(data = principal_components, columns = [['p'+str(i) for i in range(K)]])\n",
    "    \n",
    "    #recombine X_m\n",
    "    X_pca = pd.concat([X_m, pcaDf], axis=1)\n",
    "    # normalise\n",
    "    min_max_scaler = preprocessing.StandardScaler()\n",
    "    X_pca_scaled = min_max_scaler.fit_transform(X_pca)\n",
    "    X_pca_norm = pd.DataFrame(X_pca_scaled)\n",
    "\n",
    "    X_pca_norm.columns = X_pca.columns\n",
    "\n",
    "    # split into train and test data with separate labels:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca_norm,y, test_size=0.1, random_state=420)\n",
    "    \n",
    "    log_regr.fit(X_train, y_train.values.ravel())\n",
    "    plot_learning_curve(model)\n",
    "    \n",
    "    pca_accuracy.append((K, model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evalute accuracy for range of K values\n",
    "for i in range(169):\n",
    "    eval_PCA(log_regr,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57, 0.77), (49, 0.75), (50, 0.75), (51, 0.75), (53, 0.75)]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_pca_accuracy = sorted(pca_accuracy, key=lambda acc: acc[1], reverse=True)\n",
    "sorted_pca_accuracy[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like reducing the number of features with PCA does not really help the accuracy of the model, as the best accuracy with PCA is 77% with 57 principal components (which is still worse than the 78% accuracy achieved previously)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try using a Convolutional Neural Network model on this MFCC structured data**\n",
    "- Using Conv2D\n",
    "    - need to have features in a 2D array\n",
    "    \n",
    "    \n",
    "\n",
    "- This does not work well\n",
    "    - Usually deep learning techniques like CNNs are used for unstructured data like spectrograms. Therefore, the next notebook \"4-Models using Mel Spectrogram\" will dive into using the spectrograms of the audio samples to train a CNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features to 2d array 14x13\n",
    "X_2d_train = np.zeros(shape=(len(X_train), 14,13,1))\n",
    "for i in range(len(X_train)):\n",
    "    X_2d_train[i] = X_train.iloc[i].to_numpy().reshape(14,13,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 14, 13, 1)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2d_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 1s 33ms/step - loss: -14.0793 - accuracy: 0.1010 - val_loss: -72.3828 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -100.7195 - accuracy: 0.0907 - val_loss: -265.4739 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -326.3871 - accuracy: 0.1062 - val_loss: -713.1882 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -833.4907 - accuracy: 0.0914 - val_loss: -1633.7915 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 9ms/step - loss: -1819.5851 - accuracy: 0.0965 - val_loss: -3366.5708 - val_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -3698.6303 - accuracy: 0.1053 - val_loss: -6408.0581 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -6972.0616 - accuracy: 0.1053 - val_loss: -11436.4248 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -11969.9450 - accuracy: 0.1008 - val_loss: -19400.7656 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -20541.5984 - accuracy: 0.0968 - val_loss: -31459.9551 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -33583.4938 - accuracy: 0.0881 - val_loss: -49080.6094 - val_accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24da980b8b0>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,3), input_shape = X_2d_train.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten()) # as it was 2D, dense needs 1D\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_2d_train, y_train, batch_size=100, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using covariance matrix and mean matrix in CNN didn't seem to train well at all. Instead, I'll use the Mel spectogram (MFCC before carrying out DCT) as features (seen in CNN with spectrograms Notebook)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
