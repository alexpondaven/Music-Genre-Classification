{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training models on Covariance matrix and mean matrix of the MFCC of audio samples**\n",
    "\n",
    "The Covariance matrix and mean matrix was found in the Music Genre Classification Notebook, and will be used to train several models here.\n",
    "\n",
    "Models:\n",
    "- Logistic Regression?\n",
    "- Convolutional Neural Network\n",
    "    - Did not work well using the covariance matrix, so the Mel Spectrogram was used in the \"Models using Mel Spectrogram\" notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from sklearn.model_selection import train_test_split, validation_curve, learning_curve\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "import math, random, pickle, os, operator\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "## Use GPU support\n",
    "# needed to prevent error from using too much gpu memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data - could try converting to pandas Dataframe instead\n",
    "dataset = []\n",
    "train = []\n",
    "test = []\n",
    "with open(\"feat.dat\", 'rb') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            dataset.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            f.close()\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to Pandas DataFrame\n",
    "flattened = []\n",
    "for x in dataset:   \n",
    "    flattened.append(list(x[0]) + list(x[1].flatten()) + [x[2]])\n",
    "\n",
    "cols = [['m' +str(i) for i in range(13)]+['c'+str(i) for i in range(169)]+[\"Genre\"]]\n",
    "df = pd.DataFrame(flattened, columns=cols)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>m0</th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>m3</th>\n",
       "      <th>m4</th>\n",
       "      <th>m5</th>\n",
       "      <th>m6</th>\n",
       "      <th>m7</th>\n",
       "      <th>m8</th>\n",
       "      <th>m9</th>\n",
       "      <th>...</th>\n",
       "      <th>c160</th>\n",
       "      <th>c161</th>\n",
       "      <th>c162</th>\n",
       "      <th>c163</th>\n",
       "      <th>c164</th>\n",
       "      <th>c165</th>\n",
       "      <th>c166</th>\n",
       "      <th>c167</th>\n",
       "      <th>c168</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.502611</td>\n",
       "      <td>-1.961417</td>\n",
       "      <td>-15.774347</td>\n",
       "      <td>3.831419</td>\n",
       "      <td>-10.473326</td>\n",
       "      <td>1.311828</td>\n",
       "      <td>-19.393732</td>\n",
       "      <td>5.286790</td>\n",
       "      <td>-16.631725</td>\n",
       "      <td>5.353444</td>\n",
       "      <td>...</td>\n",
       "      <td>3.786696</td>\n",
       "      <td>-0.021175</td>\n",
       "      <td>-14.523117</td>\n",
       "      <td>-12.995331</td>\n",
       "      <td>-3.843489</td>\n",
       "      <td>-14.831619</td>\n",
       "      <td>2.251195</td>\n",
       "      <td>29.282607</td>\n",
       "      <td>64.650762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66.931650</td>\n",
       "      <td>0.718853</td>\n",
       "      <td>-3.267830</td>\n",
       "      <td>4.181200</td>\n",
       "      <td>-8.050941</td>\n",
       "      <td>6.448259</td>\n",
       "      <td>-17.768517</td>\n",
       "      <td>14.091810</td>\n",
       "      <td>-18.332536</td>\n",
       "      <td>3.685560</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.026806</td>\n",
       "      <td>-13.013754</td>\n",
       "      <td>18.476615</td>\n",
       "      <td>11.603178</td>\n",
       "      <td>-3.788941</td>\n",
       "      <td>-17.738734</td>\n",
       "      <td>-8.665845</td>\n",
       "      <td>18.410328</td>\n",
       "      <td>90.762876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.774103</td>\n",
       "      <td>3.039700</td>\n",
       "      <td>-19.024435</td>\n",
       "      <td>-0.983591</td>\n",
       "      <td>-14.699337</td>\n",
       "      <td>7.586562</td>\n",
       "      <td>-12.823053</td>\n",
       "      <td>1.892920</td>\n",
       "      <td>-14.865326</td>\n",
       "      <td>3.595252</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.087720</td>\n",
       "      <td>-6.627783</td>\n",
       "      <td>-2.016122</td>\n",
       "      <td>-12.890326</td>\n",
       "      <td>-3.702126</td>\n",
       "      <td>-3.862404</td>\n",
       "      <td>0.221521</td>\n",
       "      <td>-9.624113</td>\n",
       "      <td>83.777252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.696107</td>\n",
       "      <td>9.211514</td>\n",
       "      <td>-4.532876</td>\n",
       "      <td>5.027148</td>\n",
       "      <td>-8.254011</td>\n",
       "      <td>8.101201</td>\n",
       "      <td>-9.676610</td>\n",
       "      <td>9.824043</td>\n",
       "      <td>-6.735213</td>\n",
       "      <td>10.332956</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.272778</td>\n",
       "      <td>-13.864303</td>\n",
       "      <td>13.764830</td>\n",
       "      <td>4.091590</td>\n",
       "      <td>-5.564044</td>\n",
       "      <td>-11.718776</td>\n",
       "      <td>-10.097750</td>\n",
       "      <td>4.902070</td>\n",
       "      <td>69.758813</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.972846</td>\n",
       "      <td>0.506410</td>\n",
       "      <td>-23.834838</td>\n",
       "      <td>-2.181944</td>\n",
       "      <td>-29.875498</td>\n",
       "      <td>0.719521</td>\n",
       "      <td>-19.039014</td>\n",
       "      <td>-1.353935</td>\n",
       "      <td>-13.370416</td>\n",
       "      <td>5.069944</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.531564</td>\n",
       "      <td>9.558282</td>\n",
       "      <td>14.068872</td>\n",
       "      <td>-4.271712</td>\n",
       "      <td>-29.419962</td>\n",
       "      <td>13.653176</td>\n",
       "      <td>6.385477</td>\n",
       "      <td>7.264809</td>\n",
       "      <td>102.319498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          m0        m1         m2        m3         m4        m5         m6  \\\n",
       "0  76.502611 -1.961417 -15.774347  3.831419 -10.473326  1.311828 -19.393732   \n",
       "1  66.931650  0.718853  -3.267830  4.181200  -8.050941  6.448259 -17.768517   \n",
       "2  78.774103  3.039700 -19.024435 -0.983591 -14.699337  7.586562 -12.823053   \n",
       "3  66.696107  9.211514  -4.532876  5.027148  -8.254011  8.101201  -9.676610   \n",
       "4  71.972846  0.506410 -23.834838 -2.181944 -29.875498  0.719521 -19.039014   \n",
       "\n",
       "          m7         m8         m9  ...       c160       c161       c162  \\\n",
       "0   5.286790 -16.631725   5.353444  ...   3.786696  -0.021175 -14.523117   \n",
       "1  14.091810 -18.332536   3.685560  ... -16.026806 -13.013754  18.476615   \n",
       "2   1.892920 -14.865326   3.595252  ... -18.087720  -6.627783  -2.016122   \n",
       "3   9.824043  -6.735213  10.332956  ... -21.272778 -13.864303  13.764830   \n",
       "4  -1.353935 -13.370416   5.069944  ... -15.531564   9.558282  14.068872   \n",
       "\n",
       "        c163       c164       c165       c166       c167        c168 Genre  \n",
       "0 -12.995331  -3.843489 -14.831619   2.251195  29.282607   64.650762     1  \n",
       "1  11.603178  -3.788941 -17.738734  -8.665845  18.410328   90.762876     1  \n",
       "2 -12.890326  -3.702126  -3.862404   0.221521  -9.624113   83.777252     1  \n",
       "3   4.091590  -5.564044 -11.718776 -10.097750   4.902070   69.758813     1  \n",
       "4  -4.271712 -29.419962  13.653176   6.385477   7.264809  102.319498     1  \n",
       "\n",
       "[5 rows x 183 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model now has 183 features and 1000 training examples\n",
    "- 13 - mean for each cepstral coefficient\n",
    "- 169 - elements of 13x13 covariance matrix\n",
    "- 1 - genre label (1-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE ANALYSIS**\n",
    "\n",
    "\n",
    "- Split data into training and test data (and randomize)\n",
    "- normalise values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data with separate labels:\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,:-1],df[[\"Genre\"]], test_size=0.1, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation\n",
    "# standard scaler performs better than min max processor - between 0 and 1\n",
    "# - probably since it preserves the negative and positive values\n",
    "# - it may also converge quicker\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "test_scaled = min_max_scaler.fit_transform(X_test)\n",
    "# apply normalizer\n",
    "X_trainn = pd.DataFrame(train_scaled)\n",
    "X_testn = pd.DataFrame(test_scaled)\n",
    "\n",
    "X_trainn.columns = X_train.columns\n",
    "X_testn.columns = X_test.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>m0</th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>m3</th>\n",
       "      <th>m4</th>\n",
       "      <th>m5</th>\n",
       "      <th>m6</th>\n",
       "      <th>m7</th>\n",
       "      <th>m8</th>\n",
       "      <th>m9</th>\n",
       "      <th>...</th>\n",
       "      <th>c159</th>\n",
       "      <th>c160</th>\n",
       "      <th>c161</th>\n",
       "      <th>c162</th>\n",
       "      <th>c163</th>\n",
       "      <th>c164</th>\n",
       "      <th>c165</th>\n",
       "      <th>c166</th>\n",
       "      <th>c167</th>\n",
       "      <th>c168</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.306366</td>\n",
       "      <td>-0.092496</td>\n",
       "      <td>0.450666</td>\n",
       "      <td>-2.134314</td>\n",
       "      <td>0.058855</td>\n",
       "      <td>-2.768623</td>\n",
       "      <td>2.153285</td>\n",
       "      <td>-2.159789</td>\n",
       "      <td>-0.534447</td>\n",
       "      <td>-0.335421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093076</td>\n",
       "      <td>-1.447600</td>\n",
       "      <td>0.539197</td>\n",
       "      <td>0.146005</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>4.014386</td>\n",
       "      <td>-0.671173</td>\n",
       "      <td>-0.929741</td>\n",
       "      <td>1.367938</td>\n",
       "      <td>1.729033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.395286</td>\n",
       "      <td>0.292218</td>\n",
       "      <td>-0.550730</td>\n",
       "      <td>-0.755607</td>\n",
       "      <td>1.650630</td>\n",
       "      <td>-0.021541</td>\n",
       "      <td>0.419605</td>\n",
       "      <td>-0.793512</td>\n",
       "      <td>0.909649</td>\n",
       "      <td>0.293572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.612240</td>\n",
       "      <td>-0.145249</td>\n",
       "      <td>-1.850526</td>\n",
       "      <td>-0.306378</td>\n",
       "      <td>0.251588</td>\n",
       "      <td>-0.514343</td>\n",
       "      <td>0.258436</td>\n",
       "      <td>-0.299570</td>\n",
       "      <td>-0.255360</td>\n",
       "      <td>-0.192664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.487060</td>\n",
       "      <td>-1.096310</td>\n",
       "      <td>2.064278</td>\n",
       "      <td>-0.721969</td>\n",
       "      <td>1.423644</td>\n",
       "      <td>-0.352090</td>\n",
       "      <td>1.227656</td>\n",
       "      <td>0.103770</td>\n",
       "      <td>1.409154</td>\n",
       "      <td>-1.602245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735582</td>\n",
       "      <td>1.842208</td>\n",
       "      <td>0.908292</td>\n",
       "      <td>1.095844</td>\n",
       "      <td>0.723725</td>\n",
       "      <td>-0.549773</td>\n",
       "      <td>0.432835</td>\n",
       "      <td>0.849918</td>\n",
       "      <td>0.584312</td>\n",
       "      <td>0.304761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.163529</td>\n",
       "      <td>-0.229533</td>\n",
       "      <td>-0.398066</td>\n",
       "      <td>-0.385896</td>\n",
       "      <td>-0.798086</td>\n",
       "      <td>0.630567</td>\n",
       "      <td>0.397157</td>\n",
       "      <td>0.865466</td>\n",
       "      <td>-0.944766</td>\n",
       "      <td>1.568415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.790083</td>\n",
       "      <td>-1.023360</td>\n",
       "      <td>0.534273</td>\n",
       "      <td>3.119231</td>\n",
       "      <td>0.417806</td>\n",
       "      <td>-0.975491</td>\n",
       "      <td>0.653158</td>\n",
       "      <td>0.405850</td>\n",
       "      <td>-0.305321</td>\n",
       "      <td>0.118675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.442881</td>\n",
       "      <td>1.010230</td>\n",
       "      <td>-1.777626</td>\n",
       "      <td>-0.509585</td>\n",
       "      <td>-1.231931</td>\n",
       "      <td>-0.531530</td>\n",
       "      <td>-1.005433</td>\n",
       "      <td>0.289876</td>\n",
       "      <td>-0.714270</td>\n",
       "      <td>0.587926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232399</td>\n",
       "      <td>-0.442623</td>\n",
       "      <td>0.088447</td>\n",
       "      <td>0.596732</td>\n",
       "      <td>0.548695</td>\n",
       "      <td>0.456453</td>\n",
       "      <td>-1.555837</td>\n",
       "      <td>-1.156224</td>\n",
       "      <td>1.425033</td>\n",
       "      <td>1.741165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.251858</td>\n",
       "      <td>-0.321846</td>\n",
       "      <td>1.936249</td>\n",
       "      <td>-1.685662</td>\n",
       "      <td>1.256958</td>\n",
       "      <td>-1.553462</td>\n",
       "      <td>1.694849</td>\n",
       "      <td>-1.776584</td>\n",
       "      <td>0.976346</td>\n",
       "      <td>0.609899</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.786221</td>\n",
       "      <td>-0.601839</td>\n",
       "      <td>0.389283</td>\n",
       "      <td>0.940824</td>\n",
       "      <td>0.899772</td>\n",
       "      <td>0.301028</td>\n",
       "      <td>0.973186</td>\n",
       "      <td>0.312161</td>\n",
       "      <td>-0.718371</td>\n",
       "      <td>-0.562795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.820867</td>\n",
       "      <td>-0.745257</td>\n",
       "      <td>-0.046338</td>\n",
       "      <td>-1.224433</td>\n",
       "      <td>0.075097</td>\n",
       "      <td>-1.775442</td>\n",
       "      <td>0.187418</td>\n",
       "      <td>-0.823814</td>\n",
       "      <td>0.827993</td>\n",
       "      <td>-0.604918</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.865549</td>\n",
       "      <td>-0.635030</td>\n",
       "      <td>-0.417959</td>\n",
       "      <td>0.268805</td>\n",
       "      <td>0.178712</td>\n",
       "      <td>-0.708426</td>\n",
       "      <td>0.388112</td>\n",
       "      <td>1.049489</td>\n",
       "      <td>-0.894535</td>\n",
       "      <td>-1.231701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.083848</td>\n",
       "      <td>-0.731011</td>\n",
       "      <td>-0.273522</td>\n",
       "      <td>0.092869</td>\n",
       "      <td>-0.381624</td>\n",
       "      <td>1.225761</td>\n",
       "      <td>-1.138331</td>\n",
       "      <td>0.894662</td>\n",
       "      <td>-0.720826</td>\n",
       "      <td>0.708874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200482</td>\n",
       "      <td>0.486378</td>\n",
       "      <td>0.117520</td>\n",
       "      <td>0.243701</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>-0.523944</td>\n",
       "      <td>-0.101826</td>\n",
       "      <td>0.227510</td>\n",
       "      <td>-0.164058</td>\n",
       "      <td>-0.883463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>0.341144</td>\n",
       "      <td>-0.632152</td>\n",
       "      <td>1.063731</td>\n",
       "      <td>0.594703</td>\n",
       "      <td>-3.298264</td>\n",
       "      <td>-0.291018</td>\n",
       "      <td>-1.567935</td>\n",
       "      <td>-0.310687</td>\n",
       "      <td>0.531698</td>\n",
       "      <td>-1.645913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>1.055927</td>\n",
       "      <td>0.988658</td>\n",
       "      <td>0.101528</td>\n",
       "      <td>0.095468</td>\n",
       "      <td>0.348454</td>\n",
       "      <td>1.009230</td>\n",
       "      <td>0.601830</td>\n",
       "      <td>0.146006</td>\n",
       "      <td>-0.868172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>-0.318512</td>\n",
       "      <td>1.252022</td>\n",
       "      <td>-2.043336</td>\n",
       "      <td>2.466971</td>\n",
       "      <td>0.369780</td>\n",
       "      <td>-1.084946</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>-2.797913</td>\n",
       "      <td>0.257366</td>\n",
       "      <td>-2.148561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204261</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>1.135288</td>\n",
       "      <td>-0.356999</td>\n",
       "      <td>-0.476107</td>\n",
       "      <td>-0.278333</td>\n",
       "      <td>-0.312258</td>\n",
       "      <td>0.427142</td>\n",
       "      <td>-0.276237</td>\n",
       "      <td>-1.086145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           m0        m1        m2        m3        m4        m5        m6  \\\n",
       "0    0.306366 -0.092496  0.450666 -2.134314  0.058855 -2.768623  2.153285   \n",
       "1    0.395286  0.292218 -0.550730 -0.755607  1.650630 -0.021541  0.419605   \n",
       "2    0.487060 -1.096310  2.064278 -0.721969  1.423644 -0.352090  1.227656   \n",
       "3    0.163529 -0.229533 -0.398066 -0.385896 -0.798086  0.630567  0.397157   \n",
       "4   -1.442881  1.010230 -1.777626 -0.509585 -1.231931 -0.531530 -1.005433   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "895  0.251858 -0.321846  1.936249 -1.685662  1.256958 -1.553462  1.694849   \n",
       "896  0.820867 -0.745257 -0.046338 -1.224433  0.075097 -1.775442  0.187418   \n",
       "897  0.083848 -0.731011 -0.273522  0.092869 -0.381624  1.225761 -1.138331   \n",
       "898  0.341144 -0.632152  1.063731  0.594703 -3.298264 -0.291018 -1.567935   \n",
       "899 -0.318512  1.252022 -2.043336  2.466971  0.369780 -1.084946  0.269200   \n",
       "\n",
       "           m7        m8        m9  ...      c159      c160      c161  \\\n",
       "0   -2.159789 -0.534447 -0.335421  ...  0.093076 -1.447600  0.539197   \n",
       "1   -0.793512  0.909649  0.293572  ...  0.612240 -0.145249 -1.850526   \n",
       "2    0.103770  1.409154 -1.602245  ...  0.735582  1.842208  0.908292   \n",
       "3    0.865466 -0.944766  1.568415  ... -0.790083 -1.023360  0.534273   \n",
       "4    0.289876 -0.714270  0.587926  ...  0.232399 -0.442623  0.088447   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "895 -1.776584  0.976346  0.609899  ... -0.786221 -0.601839  0.389283   \n",
       "896 -0.823814  0.827993 -0.604918  ... -0.865549 -0.635030 -0.417959   \n",
       "897  0.894662 -0.720826  0.708874  ...  0.200482  0.486378  0.117520   \n",
       "898 -0.310687  0.531698 -1.645913  ...  0.005248  1.055927  0.988658   \n",
       "899 -2.797913  0.257366 -2.148561  ... -0.204261  0.101200  1.135288   \n",
       "\n",
       "         c162      c163      c164      c165      c166      c167      c168  \n",
       "0    0.146005  0.004481  4.014386 -0.671173 -0.929741  1.367938  1.729033  \n",
       "1   -0.306378  0.251588 -0.514343  0.258436 -0.299570 -0.255360 -0.192664  \n",
       "2    1.095844  0.723725 -0.549773  0.432835  0.849918  0.584312  0.304761  \n",
       "3    3.119231  0.417806 -0.975491  0.653158  0.405850 -0.305321  0.118675  \n",
       "4    0.596732  0.548695  0.456453 -1.555837 -1.156224  1.425033  1.741165  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "895  0.940824  0.899772  0.301028  0.973186  0.312161 -0.718371 -0.562795  \n",
       "896  0.268805  0.178712 -0.708426  0.388112  1.049489 -0.894535 -1.231701  \n",
       "897  0.243701  0.023057 -0.523944 -0.101826  0.227510 -0.164058 -0.883463  \n",
       "898  0.101528  0.095468  0.348454  1.009230  0.601830  0.146006 -0.868172  \n",
       "899 -0.356999 -0.476107 -0.278333 -0.312258  0.427142 -0.276237 -1.086145  \n",
       "\n",
       "[900 rows x 182 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Models</strong>\n",
    "\n",
    "**Logistic Regression**\n",
    "- Accuracy of 71% using:\n",
    "LogisticRegression(random_state=2, solver='lbfgs', multi_class='ovr', max_iter=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=8000, multi_class='ovr', n_jobs=-1, random_state=2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit model\n",
    "# lbfgs solver used for multiclass problem - quicker than saga\n",
    "log_regr = LogisticRegression(n_jobs=-1,random_state=2, solver='lbfgs', multi_class='ovr', max_iter=8000)\n",
    "log_regr.fit(X_trainn, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 182)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_regr.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using validation curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(log_regr, X_trainn, y_train.values.ravel(),\n",
    "                                                        train_sizes=np.linspace(150,700,50, dtype=int),cv=5,\n",
    "                                                        scoring = 'neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores_mean = -train_scores.mean(axis=1)\n",
    "valid_scores_mean =-valid_scores.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.03157895, -0.        ,  0.03585657,\n",
       "        0.03435115,  0.03956044,  0.03169014,  0.03050847,  0.02931596,\n",
       "        0.02830189,  0.02735562,  0.04764706,  0.17045455,  0.22754821,\n",
       "        0.25240642,  0.23688312,  0.29343434,  0.3495098 ,  0.38377088,\n",
       "        0.55209302,  0.55646259,  0.52626932,  0.59396552,  0.62021053,\n",
       "        0.6090535 ,  0.74486922,  0.7654224 ,  0.78923077,  0.81054614,\n",
       "        0.8402214 ,  0.87725632,  0.87929204,  0.87013889,  0.90630324,\n",
       "        0.9264214 ,  1.04196721,  1.02512077,  1.06962025,  1.08802488,\n",
       "        1.17465649,  1.20810811,  1.22895126,  1.23226744,  1.25771429])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1986c0ecca0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curves\n",
    "plt.plot(train_sizes, train_scores_mean, label = \"Training error\")\n",
    "plt.plot(train_sizes, valid_scores_mean, label = \"Validation error\")\n",
    "plt.xlabel(\"Training size\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Learning Curve Logistic Regression Model\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the learning curve above, it is apparent that more training data would help the validation curve converge towards the training curve and decrease error. This is done in the \"Adding more data\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "def accuracy(model):\n",
    "    pred = model.predict(X_testn)\n",
    "    return round(sum(y_test.values.ravel() == pred) / len(y_test), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy:  0.71\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of logistic Regression model\n",
    "print(\"Logistic Regression Model Accuracy: \", accuracy(log_regr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_regr.score(X_testn, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Convolutional Neural Network model**\n",
    "- Using Conv2D\n",
    "    - need to have features in a 2D array\n",
    "        - can append \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features to 2d array 14x13\n",
    "X_2d_train = np.zeros(shape=(len(X_trainn), 14,13,1))\n",
    "for i in range(len(X_trainn)):\n",
    "    X_2d_train[i] = X_trainn.iloc[i].to_numpy().reshape(14,13,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 14, 13, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2d_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 5s 154ms/step - loss: -2.6867 - accuracy: 0.0933 - val_loss: -18.3856 - val_accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -28.2873 - accuracy: 0.1064 - val_loss: -82.6419 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -113.5675 - accuracy: 0.0999 - val_loss: -266.1078 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -338.0009 - accuracy: 0.0942 - val_loss: -708.6118 - val_accuracy: 0.1000\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -871.5696 - accuracy: 0.1068 - val_loss: -1643.1027 - val_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -2000.9289 - accuracy: 0.0914 - val_loss: -3425.5278 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -3959.8122 - accuracy: 0.1023 - val_loss: -6558.2432 - val_accuracy: 0.1000\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -7633.7029 - accuracy: 0.0962 - val_loss: -11724.6602 - val_accuracy: 0.1000\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -13105.3041 - accuracy: 0.1049 - val_loss: -19700.8301 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: -22032.3584 - accuracy: 0.1000 - val_loss: -31718.2949 - val_accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19832c38550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,3), input_shape = X_2d_train.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten()) # as it was 2D, dense needs 1D\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_2d_train, y_train, batch_size=100, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using covariance matrix and mean matrix in CNN didn't seem to train well at all. Instead, I'll use the Mel spectogram (MFCC before carrying out DCT) as features (seen in CNN with spectrograms Notebook).\n",
    "Otherwise, I could use a different model for the mel coefficients like logistic regression, multiclass SVMs and K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 182)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-2982848bd061>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np.array(dataset).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dataset).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
